{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing non-curated tabular datasets\n",
    "Example of making a dataset that is not curated by fastai available for training a fastai deep learning application.\n",
    "\n",
    "In this notebook we'll go through the steps in ingest the Kaggle house prices dataset: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastdownload in /opt/conda/envs/fastai/lib/python3.8/site-packages (0.0.5)\n",
      "Requirement already satisfied: fastprogress in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastdownload) (1.0.0)\n",
      "Requirement already satisfied: fastcore>=1.3.26 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastdownload) (1.3.26)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastprogress->fastdownload) (1.19.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastcore>=1.3.26->fastdownload) (20.4)\n",
      "Requirement already satisfied: pip in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastcore>=1.3.26->fastdownload) (20.2.4)\n",
      "Requirement already satisfied: six in /opt/conda/envs/fastai/lib/python3.8/site-packages (from packaging->fastcore>=1.3.26->fastdownload) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from packaging->fastcore>=1.3.26->fastdownload) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "# imports for notebook boilerplate\n",
    "!pip install -Uqq fastbook\n",
    "!pip install fastdownload\n",
    "import fastbook\n",
    "from fastbook import *\n",
    "from fastai.tabular.all import *\n",
    "from fastdownload import FastDownload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required for this notebook\n",
    "from kaggle import api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the notebook for fast.ai\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing a Kaggle dataset\n",
    "\n",
    "The following cells assume that you have completed the following steps:\n",
    "- Created a Kaggle ID, if you don't already have one: https://www.kaggle.com/account/login\n",
    "- Log into your Kaggle ID and go through the steps to download your Kaggle API key file: kaggle.json\n",
    "- Uploaded your kaggle.json file to the directory /root/.kaggle in your Gradient instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the contents of your kaggle.json file into creds\n",
    "creds = '{\"username\":<YOUR ID>,\"key\":<YOUR KEY>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the kaggle credentials path\n",
    "cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a target path for this house price dataset\n",
    "path = URLs.path('house_price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need an explicit definition of file_extract\n",
    "def file_extract(fname, dest=None):\n",
    "    \"Extract `fname` to `dest` using `tarfile` or `zipfile`.\"\n",
    "    if dest is None: dest = Path(fname).parent\n",
    "    fname = str(fname)\n",
    "    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n",
    "    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n",
    "    else: raise Exception(f'Unrecognized archive: {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199k/199k [00:00<00:00, 5.69MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading house-prices-advanced-regression-techniques.zip to /root/.fastai/archive/house_price\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#4) [Path('/root/.fastai/archive/house_price/train.csv'),Path('/root/.fastai/archive/house_price/test.csv'),Path('/root/.fastai/archive/house_price/sample_submission.csv'),Path('/root/.fastai/archive/house_price/data_description.txt')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the target path for the dataset and copy it into /storage/archive on Gradient\n",
    "if not path.exists():\n",
    "    print('path does not exist')\n",
    "    path.mkdir()\n",
    "    api.competition_download_cli('house-prices-advanced-regression-techniques', path=path)\n",
    "    #d = FastDownload()\n",
    "    #d.get(path/'house-prices-advanced-regression-techniques.zip')\n",
    "    file_extract(path/'house-prices-advanced-regression-techniques.zip')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# list the directory structure of the newly created dataset\n",
    "path.ls(file_type='text')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest and explore the dataset\n",
    "In this dataset the train and test subsets are in separate CSV files. Ingest each of these and explore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest the dataset into a Pandas dataframe\n",
    "df_train = pd.read_csv(path/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path/'test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the shape of test - why does it have one less column than the train dataset?\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set target\n",
    "adjust target column for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace target values with value indicating whether the input is over or under the mean\n",
    "def under_over(x,mean_x):\n",
    "    if (x <= mean_x):\n",
    "        returner = '0'\n",
    "    else:\n",
    "        returner = '1'\n",
    "    return(returner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average of the values in the SalePrice column\n",
    "mean_sp = int(df_train['SalePrice'].mean())\n",
    "# use the under_over() function to replace the values in the SalePrice column with indicators whether the value was over or under\n",
    "# the average for the SalePrice column\n",
    "df_train['SalePrice'] = df_train['SalePrice'].apply(lambda x: under_over(x,mean_sp))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['SalePrice'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define target, categorical and continuous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dependent variable (y value)\n",
    "dep_var = 'SalePrice'\n",
    "# define columns that are continuous / categorical\n",
    "cont,cat = cont_cat_split(df_train, 1, dep_var=dep_var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len cont is \",len(cont))\n",
    "print(\"len cat is \",len(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[cat].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe that has rows for each column in df_train with missing values and \n",
    "# columns for the count and ratio of missing values\n",
    "count = df_train.isna().sum()\n",
    "df_train_missing = (pd.concat([count.rename('missing_count'),\n",
    "                     count.div(len(df_train))\n",
    "                          .rename('missing_ratio')],axis = 1)\n",
    "             .loc[count.ne(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count2 = df_test.isna().sum()\n",
    "df_test_missing = (pd.concat([count2.rename('missing_count'),\n",
    "                     count2.div(len(df_test))\n",
    "                          .rename('missing_ratio')],axis = 1)\n",
    "             .loc[count2.ne(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see missing value col count in test set\n",
    "df_test_missing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for categorical columns, replace missing values with the most column categorical value in that column\n",
    "df_train[cat] = df_train[cat].fillna(df_train[cat].mode().iloc[0])\n",
    "df_test[cat] = df_test[cat].fillna(df_test[cat].mode().iloc[0])\n",
    "# for continuous columns, replace missing values with 0\n",
    "df_train[cont] = df_train[cont].fillna(0.0)\n",
    "df_test[cont] = df_test[cont].fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm missing values dealt with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in df_train\n",
    "count = df_train.isna().sum()\n",
    "df_train_missing = (pd.concat([count.rename('missing_count'),\n",
    "                     count.div(len(df_train))\n",
    "                          .rename('missing_ratio')],axis = 1)\n",
    "             .loc[count.ne(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in df_test\n",
    "count = df_test.isna().sum()\n",
    "df_test_missing = (pd.concat([count.rename('missing_count'),\n",
    "                     count.div(len(df_test))\n",
    "                          .rename('missing_ratio')],axis = 1)\n",
    "             .loc[count.ne(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define TabularDataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define TabularDataLoaders object \n",
    "# valid_idx: the indices to use for the validation set\n",
    "# what happens when we try to run this without dealing with missing values first\n",
    "procs = [Categorify, Normalize]\n",
    "dls_house=TabularDataLoaders.from_df(\n",
    "    df_train,path,procs= procs,\n",
    "    cat_names= cat, cont_names = cont, y_names = dep_var, \n",
    "    valid_idx=list(range((df_train.shape[0]-100),df_train.shape[0])), \n",
    "    bs=64)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_house.valid.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the model\n",
    "learn = tabular_learner(dls_house, layers=[200,100], metrics=accuracy)\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply trained model to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to the test set\n",
    "# details of test_dl here: https://docs.fast.ai/tutorial.tabular\n",
    "dl = learn.dls.test_dl(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.get_preds(dl=dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the structure of the trained model structure\n",
    "Use the summary() function to see the structure of the trained model, including:\n",
    "- the layers that make up the model\n",
    "- total parameters\n",
    "- loss function\n",
    "- optimizer function\n",
    "- callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
